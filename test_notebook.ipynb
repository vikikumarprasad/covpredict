{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "978af9b4-53d8-487d-8372-7c5dfc464058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: datamol in /opt/miniconda3/lib/python3.12/site-packages (0.12.4)\n",
      "Requirement already satisfied: rdkit in /opt/miniconda3/lib/python3.12/site-packages (2023.9.6)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/lib/python3.12/site-packages (1.5.0)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.2.5-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting ngboost\n",
      "  Downloading ngboost-0.5.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.12/site-packages (from datamol) (4.65.0)\n",
      "Requirement already satisfied: loguru in /opt/miniconda3/lib/python3.12/site-packages (from datamol) (0.7.2)\n",
      "Requirement already satisfied: fsspec>=2021.9 in /opt/miniconda3/lib/python3.12/site-packages (from datamol) (2024.5.0)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/lib/python3.12/site-packages (from datamol) (1.13.1)\n",
      "Requirement already satisfied: matplotlib in /opt/miniconda3/lib/python3.12/site-packages (from datamol) (3.9.0)\n",
      "Requirement already satisfied: pillow in /opt/miniconda3/lib/python3.12/site-packages (from datamol) (10.3.0)\n",
      "Requirement already satisfied: selfies in /opt/miniconda3/lib/python3.12/site-packages (from datamol) (2.1.1)\n",
      "Requirement already satisfied: platformdirs in /opt/miniconda3/lib/python3.12/site-packages (from datamol) (3.10.0)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.12/site-packages (from datamol) (23.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/miniconda3/lib/python3.12/site-packages (from datamol) (4.11.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/miniconda3/lib/python3.12/site-packages (from datamol) (6.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting plotly (from catboost)\n",
      "  Downloading plotly-5.22.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: six in /opt/miniconda3/lib/python3.12/site-packages (from catboost) (1.16.0)\n",
      "Collecting lifelines>=0.25 (from ngboost)\n",
      "  Downloading lifelines-0.28.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting autograd>=1.5 (from lifelines>=0.25->ngboost)\n",
      "  Downloading autograd-1.6.2-py3-none-any.whl.metadata (706 bytes)\n",
      "Collecting autograd-gamma>=0.3 (from lifelines>=0.25->ngboost)\n",
      "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting formulaic>=0.2.2 (from lifelines>=0.25->ngboost)\n",
      "  Downloading formulaic-1.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib->datamol) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib->datamol) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib->datamol) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib->datamol) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib->datamol) (3.1.2)\n",
      "Collecting tenacity>=6.2.0 (from plotly->catboost)\n",
      "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting future>=0.15.2 (from autograd>=1.5->lifelines>=0.25->ngboost)\n",
      "  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines>=0.25->ngboost)\n",
      "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting wrapt>=1.0 (from formulaic>=0.2.2->lifelines>=0.25->ngboost)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading catboost-1.2.5-cp312-cp312-manylinux2014_x86_64.whl (98.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ngboost-0.5.1-py3-none-any.whl (33 kB)\n",
      "Downloading lifelines-0.28.0-py3-none-any.whl (349 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.2/349.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading plotly-5.22.0-py3-none-any.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading autograd-1.6.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading formulaic-1.0.1-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: autograd-gamma\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=dd1fcbfc25a787c5c2bb34dea4094c57add93da7dfd8778017ef920ae669b104\n",
      "  Stored in directory: /home/vikikrpd/.cache/pip/wheels/50/37/21/0a719b9d89c635e89ff24bd93b862882ad675279552013b2fb\n",
      "Successfully built autograd-gamma\n",
      "Installing collected packages: wrapt, tenacity, interface-meta, graphviz, future, xgboost, plotly, lightgbm, autograd, formulaic, catboost, autograd-gamma, lifelines, ngboost\n",
      "Successfully installed autograd-1.6.2 autograd-gamma-0.5.0 catboost-1.2.5 formulaic-1.0.1 future-1.0.0 graphviz-0.20.3 interface-meta-1.3.0 lifelines-0.28.0 lightgbm-4.3.0 ngboost-0.5.1 plotly-5.22.0 tenacity-8.3.0 wrapt-1.16.0 xgboost-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy joblib datamol rdkit scikit-learn xgboost lightgbm catboost ngboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "2566399e-93b1-46b7-8372-6dd05feedfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datamol as dm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors, AllChem\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GroupShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c890a2ca-2f1a-4503-b801-f047291fcdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_variance_features(data):\n",
    "    selector = VarianceThreshold()\n",
    "    selector.fit(data)\n",
    "    retained_columns =  data.columns[selector.get_support()]\n",
    "    dropped_columns =  data.columns[~selector.get_support()]\n",
    "    print(f\"{len(dropped_columns)} columns dropped because of zero variance features: {dropped_columns.tolist()}\")\n",
    "    return data[retained_columns]\n",
    "\n",
    "def remove_collinear_features(data, threshold):\n",
    "    corr_matrix = data.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "    print(f\"{len(to_drop)} columns dropped because of correlation greater than {threshold}: {to_drop}\")\n",
    "    return data.drop(columns=to_drop, axis=1, inplace=False)\n",
    "\n",
    "def scale_features_standard(data):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    return pd.DataFrame(scaled_data, columns=data.columns)\n",
    "\n",
    "def scale_features_minmax(data):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    return pd.DataFrame(scaled_data, columns=data.columns)\n",
    "\n",
    "def scale_targets_standard(data):\n",
    "    scaler = StandardScaler()\n",
    "    data_array = data.values.reshape(-1, 1)\n",
    "    scaled_data = scaler.fit_transform(data_array)\n",
    "    return pd.DataFrame(scaled_data, columns=[data.columns]), scaler\n",
    "\n",
    "def scale_targets_minmax(data):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    data_array = data.values.reshape(-1, 1)\n",
    "    scaled_data = scaler.fit_transform(data_array)\n",
    "    return pd.DataFrame(scaled_data, columns=[data.columns]), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "661ed454-5cc3-441c-8b69-37fd828c91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X1_reactant = pd.read_csv('reactantfeats_morfeusdesc.csv', index_col=False)\n",
    "df_X1_ts = pd.read_csv('tsfeats_morfeusdesc.csv', index_col=False)\n",
    "df_X1_product = pd.read_csv('productfeats_morfeusdesc.csv', index_col=False)\n",
    "df_X2_reactant = pd.read_csv('reactantfeats_mordreddesc.csv', usecols=['Datapoint','TPSA','PEOE_VSA1','PEOE_VSA2','PEOE_VSA3','PEOE_VSA4','PEOE_VSA5','PEOE_VSA6','PEOE_VSA7','PEOE_VSA8','PEOE_VSA9','PEOE_VSA10','PEOE_VSA11','PEOE_VSA12','PEOE_VSA13','SMR_VSA1','SMR_VSA2','SMR_VSA3','SMR_VSA4','SMR_VSA5','SMR_VSA6','SMR_VSA7','SMR_VSA8','SMR_VSA9','SlogP_VSA1','SlogP_VSA2','SlogP_VSA3','SlogP_VSA4','SlogP_VSA5','SlogP_VSA6','SlogP_VSA7','SlogP_VSA8','SlogP_VSA9','SlogP_VSA10','SlogP_VSA11','EState_VSA1','EState_VSA2','EState_VSA3','EState_VSA4','EState_VSA5','EState_VSA6','EState_VSA7','EState_VSA8','EState_VSA9','EState_VSA10','VSA_EState1','VSA_EState2','VSA_EState3','VSA_EState4','VSA_EState5','VSA_EState6','VSA_EState7','VSA_EState8','VSA_EState9'], index_col=False)\n",
    "df_X2_ts = pd.read_csv('tsfeats_mordreddesc.csv', usecols=['Datapoint','TPSA','PEOE_VSA1','PEOE_VSA2','PEOE_VSA3','PEOE_VSA4','PEOE_VSA5','PEOE_VSA6','PEOE_VSA7','PEOE_VSA8','PEOE_VSA9','PEOE_VSA10','PEOE_VSA11','PEOE_VSA12','PEOE_VSA13','SMR_VSA1','SMR_VSA2','SMR_VSA3','SMR_VSA4','SMR_VSA5','SMR_VSA6','SMR_VSA7','SMR_VSA8','SMR_VSA9','SlogP_VSA1','SlogP_VSA2','SlogP_VSA3','SlogP_VSA4','SlogP_VSA5','SlogP_VSA6','SlogP_VSA7','SlogP_VSA8','SlogP_VSA9','SlogP_VSA10','SlogP_VSA11','EState_VSA1','EState_VSA2','EState_VSA3','EState_VSA4','EState_VSA5','EState_VSA6','EState_VSA7','EState_VSA8','EState_VSA9','EState_VSA10','VSA_EState1','VSA_EState2','VSA_EState3','VSA_EState4','VSA_EState5','VSA_EState6','VSA_EState7','VSA_EState8','VSA_EState9'], index_col=False)\n",
    "df_X2_product = pd.read_csv('productfeats_mordreddesc.csv', usecols=['Datapoint','TPSA','PEOE_VSA1','PEOE_VSA2','PEOE_VSA3','PEOE_VSA4','PEOE_VSA5','PEOE_VSA6','PEOE_VSA7','PEOE_VSA8','PEOE_VSA9','PEOE_VSA10','PEOE_VSA11','PEOE_VSA12','PEOE_VSA13','SMR_VSA1','SMR_VSA2','SMR_VSA3','SMR_VSA4','SMR_VSA5','SMR_VSA6','SMR_VSA7','SMR_VSA8','SMR_VSA9','SlogP_VSA1','SlogP_VSA2','SlogP_VSA3','SlogP_VSA4','SlogP_VSA5','SlogP_VSA6','SlogP_VSA7','SlogP_VSA8','SlogP_VSA9','SlogP_VSA10','SlogP_VSA11','EState_VSA1','EState_VSA2','EState_VSA3','EState_VSA4','EState_VSA5','EState_VSA6','EState_VSA7','EState_VSA8','EState_VSA9','EState_VSA10','VSA_EState1','VSA_EState2','VSA_EState3','VSA_EState4','VSA_EState5','VSA_EState6','VSA_EState7','VSA_EState8','VSA_EState9'], index_col=False)\n",
    "#df_X3_reactant = pd.read_csv('wrtprecomplex_refDZ.csv', usecols=['Datapoint', 'eR', 'GR'], index_col=False)\n",
    "#df_X3_ts = pd.read_csv('wrtprecomplex_refDZ.csv', usecols=['Datapoint', 'eTS', 'GTS'], index_col=False)\n",
    "#df_X3_product = pd.read_csv('wrtprecomplex_refDZ.csv', usecols=['Datapoint', 'eP', 'GP'], index_col=False)\n",
    "#df_X3_reactant.rename(columns={'eR': 'e_refDZ', 'GR': 'G_refDZ'}, inplace=True)\n",
    "#df_X3_ts.rename(columns={'eTS': 'e_refDZ', 'GTS': 'G_refDZ'}, inplace=True)\n",
    "#df_X3_product.rename(columns={'eP': 'e_refDZ', 'GP': 'G_refDZ'}, inplace=True)\n",
    "df_X3_reactant = pd.read_csv('wrtprecomplex_pm7.csv', usecols=['Datapoint', 'eR', 'GR', 'HR', 'freqR', 'irR', 'dipoleR'], index_col=False)\n",
    "df_X3_ts = pd.read_csv('wrtprecomplex_pm7.csv', usecols=['Datapoint', 'eTS', 'GTS', 'HTS', 'freqTS', 'irTS', 'dipoleTS'], index_col=False)\n",
    "df_X3_product = pd.read_csv('wrtprecomplex_pm7.csv', usecols=['Datapoint', 'eP', 'GP', 'HP', 'freqP', 'irP', 'dipoleP'], index_col=False)\n",
    "df_X3_reactant.rename(columns={'eR': 'e_PM7', 'GR': 'G_PM7', 'HR': 'H_PM7', 'freqR': 'freq_PM7', 'irR': 'ir_PM7', 'dipoleR': 'dipole_PM7'}, inplace=True)\n",
    "df_X3_ts.rename(columns={'eTS': 'e_PM7', 'GTS': 'G_PM7', 'HTS': 'H_PM7', 'freqTS': 'freq_PM7', 'irTS': 'ir_PM7', 'dipoleTS': 'dipole_PM7'}, inplace=True)\n",
    "df_X3_product.rename(columns={'eP': 'e_PM7', 'GP': 'G_PM7', 'HP': 'H_PM7', 'freqP': 'freq_PM7', 'irP': 'ir_PM7', 'dipoleP': 'dipole_PM7'}, inplace=True)\n",
    "\n",
    "\n",
    "df_X_reactant = pd.merge(df_X1_reactant, df_X3_reactant, on='Datapoint', how='inner')\n",
    "df_X_ts = pd.merge(df_X1_ts, df_X3_ts, on='Datapoint', how='inner')\n",
    "df_X_product = pd.merge(df_X1_product, df_X3_product, on='Datapoint', how='inner')\n",
    "#df_Xtmp_reactant = pd.merge(df_X1_reactant, df_X2_reactant, on='Datapoint', how='inner')\n",
    "#df_Xtmp_ts = pd.merge(df_X1_ts, df_X2_ts, on='Datapoint', how='inner')\n",
    "#df_Xtmp_product = pd.merge(df_X1_product, df_X2_product, on='Datapoint', how='inner')\n",
    "#df_X_reactant = pd.merge(df_Xtmp_reactant, df_X3_reactant, on='Datapoint', how='inner')\n",
    "#df_X_ts = pd.merge(df_Xtmp_ts, df_X3_ts, on='Datapoint', how='inner')\n",
    "#df_X_product = pd.merge(df_Xtmp_product, df_X3_product, on='Datapoint', how='inner')\n",
    "\n",
    "\n",
    "df_X_reactant.sort_values(by='Datapoint', inplace=True)\n",
    "df_X_product.sort_values(by='Datapoint', inplace=True)\n",
    "common_datapoints = set(df_X_reactant['Datapoint']).intersection(set(df_X_product['Datapoint']))\n",
    "df_X_reactant = df_X_reactant[df_X_reactant['Datapoint'].isin(common_datapoints)]\n",
    "df_X_product = df_X_product[df_X_product['Datapoint'].isin(common_datapoints)]\n",
    "df_X_reactant.reset_index(drop=True, inplace=True)\n",
    "df_X_product.reset_index(drop=True, inplace=True)\n",
    "df_X = df_X_product.drop(columns=['Datapoint']) - df_X_reactant.drop(columns=['Datapoint'])\n",
    "\n",
    "#df_X_reactant.sort_values(by='Datapoint', inplace=True)\n",
    "#df_X_ts.sort_values(by='Datapoint', inplace=True)\n",
    "#common_datapoints = set(df_X_reactant['Datapoint']).intersection(set(df_X_ts['Datapoint']))\n",
    "#df_X_reactant = df_X_reactant[df_X_reactant['Datapoint'].isin(common_datapoints)]\n",
    "#df_X_ts = df_X_ts[df_X_ts['Datapoint'].isin(common_datapoints)]\n",
    "#df_X_reactant.reset_index(drop=True, inplace=True)\n",
    "#df_X_ts.reset_index(drop=True, inplace=True)\n",
    "#df_X = df_X_ts.drop(columns=['Datapoint']) - df_X_reactant.drop(columns=['Datapoint']) \n",
    "\n",
    "df_X['Datapoint'] = df_X_reactant['Datapoint']\n",
    "cols = ['Datapoint'] + [col for col in df_X.columns if col != 'Datapoint']\n",
    "df_X = df_X[cols]\n",
    "\n",
    "#df_X = df_X_product\n",
    "\n",
    "df_y = pd.read_csv('refDZ+PM7.csv', usecols=['Datapoint', 'deTSR_refDZ', 'dGTSR_refDZ', 'dGTSP_refDZ', 'dGTSR_PM7', 'dGTSP_PM7'], index_col=False)\n",
    "df_merged = pd.merge(df_X, df_y, on='Datapoint', how='inner')\n",
    "X = df_merged.drop(['Datapoint', 'deTSR_refDZ', 'dGTSR_refDZ', 'dGTSP_refDZ', 'dGTSR_PM7', 'dGTSP_PM7'], axis=1)\n",
    "\n",
    "#y_ref = df_merged['dGTSR_refDZ']\n",
    "#y_sqm = df_merged['dGTSR_PM7']\n",
    "#sqm_mae = mean_absolute_error(y_ref, y_sqm)\n",
    "#print(sqm_mae)\n",
    "\n",
    "y_ref = pd.DataFrame()\n",
    "y_sqm = pd.DataFrame()\n",
    "y_diff = pd.DataFrame()\n",
    "#y_ref['deTSR_refDZ'] = df_merged['deTSR_refDZ']\n",
    "y_ref['Datapoint'] = df_merged['Datapoint']\n",
    "y_sqm['Datapoint'] = df_merged['Datapoint']\n",
    "y_diff['Datapoint'] = df_merged['Datapoint']\n",
    "y_ref['dGTSR_refDZ'] = df_merged['dGTSR_refDZ']\n",
    "y_sqm['dGTSR_PM7'] = df_merged['dGTSR_PM7']\n",
    "y_diff['dGTSR_diff'] = df_merged['dGTSR_PM7'] - df_merged['dGTSR_refDZ']\n",
    "#y_ref['dGTSP_refDZ'] = df_merged['dGTSP_refDZ']\n",
    "#y_sqm['dGTSP_PM7'] = df_merged['dGTSP_PM7']\n",
    "#y_diff['dTSP_diff'] = df_merged['dGTSP_PM7'] - df_merged['dGTSP_refDZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "d1f10ae5-e9d2-4aae-bd71-6d118496e8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 columns dropped because of zero variance features: []\n",
      "0 columns dropped because of correlation greater than 0.99: []\n",
      "(748, 13)\n",
      "(188, 13)\n",
      "(748, 1)\n",
      "(188, 1)\n",
      "['SASA', 'SASV', 'FracBuriedVol', 'SterimolB1', 'SterimolB5', 'SterimolL', 'Pint', 'e_PM7', 'G_PM7', 'H_PM7', 'freq_PM7', 'ir_PM7', 'dipole_PM7']\n"
     ]
    }
   ],
   "source": [
    "X_numeric = X.select_dtypes(include=[np.number])  # Ensure only numeric data is processed\n",
    "X = remove_zero_variance_features(X_numeric) # Removing zero variance threshold features\n",
    "X = remove_collinear_features(X, 0.99) # Removing collinear features\n",
    "X = scale_features_standard(X) # Normalization of features\n",
    "#X = scale_features_minmax(X)\n",
    "#print(X.describe())\n",
    "\n",
    "#y = y_ref\n",
    "#y = y_diff\n",
    "#y, y_scaler = scale_targets_standard(y)\n",
    "#y, y_scaler = scale_targets_minmax(y)\n",
    "#joblib.dump(y_scaler, 'y_scaler-mordred2.pkl') #Save the y_scaler for inverse transform predictions later\n",
    "#print(y.describe())\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "combined_df = pd.concat([X, y_ref, y_sqm, y_diff], axis=1)\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train = train_df.drop(columns=['Datapoint','dGTSR_refDZ', 'dGTSR_PM7','dGTSR_diff'])\n",
    "X_test = test_df.drop(columns=['Datapoint', 'dGTSR_refDZ', 'dGTSR_PM7', 'dGTSR_diff'])\n",
    "\n",
    "ydiff_train = pd.DataFrame()\n",
    "ydiff_test = pd.DataFrame()\n",
    "yref_train = pd.DataFrame()\n",
    "yref_test = pd.DataFrame()\n",
    "ysqm_train = pd.DataFrame()\n",
    "ysqm_test = pd.DataFrame()\n",
    "ydiff_train['dTSR_diff'] = train_df['dGTSR_diff']\n",
    "ydiff_test['dTSRdiff'] = test_df['dGTSR_diff']\n",
    "yref_train['dGTSR_refDZ'] = train_df['dGTSR_refDZ']\n",
    "yref_test['dGTSR_refDZ'] = test_df['dGTSR_refDZ']\n",
    "ysqm_train['dGTSR_PM7'] = train_df['dGTSR_PM7']\n",
    "ysqm_test['dGTSR_PM7'] = test_df['dGTSR_PM7']\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(ydiff_train.shape)\n",
    "print(ydiff_test.shape)\n",
    "select_columns = [column for column in X_train.columns]\n",
    "print(select_columns)\n",
    "\n",
    "#print(yref_train.values.ravel())\n",
    "#print(ysqm_train.values.ravel())\n",
    "#print(ydiff_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "8dbe4708-01ad-4ebd-bbc9-60635629b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold, LeaveOneOut\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge, PassiveAggressiveRegressor, HuberRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from ngboost import NGBRegressor\n",
    "\n",
    "from warnings import simplefilter, filterwarnings\n",
    "from sklearn.exceptions import ConvergenceWarning, DataConversionWarning\n",
    "simplefilter('ignore', category=ConvergenceWarning)\n",
    "#simplefilter('ignore', category=DataConversionWarning)\n",
    "filterwarnings('ignore', category=UserWarning, module='sklearn.linear_model._ridge')\n",
    "\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct, WhiteKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "b2606586-74d2-4b02-b614-5072b4300d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(model_name, model_class, X_train, X_test, ydiff_train, ydiff_test, yref_train, yref_test, ysqm_train, ysqm_test):\n",
    "    y_train = yref_train.values.ravel()\n",
    "    y_test = yref_test.values.ravel()\n",
    "    #y_train = ydiff_train.values.ravel()\n",
    "    #y_test = ydiff_test.values.ravel()\n",
    "    print(f\"Fitting {model_name}...\")\n",
    "    if model_name == 'CatBoostRegressor':\n",
    "        model = model_class(verbose=0)\n",
    "    elif model_name == 'GaussianProcessRegressor':\n",
    "        #kernel=ConstantKernel(1.0) * RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0)\n",
    "        kernel = 1.0 * RationalQuadratic(length_scale=1.0, alpha=0.1)\n",
    "        alpha = 10.0\n",
    "        model = model_class(kernel=kernel, alpha=alpha)\n",
    "    else:\n",
    "        model = model_class()\n",
    "    model.fit(X_train, y_train)\n",
    "    #joblib.dump(model, f'{model_name}_model.pkl')\n",
    "    print(f\"Model fitting complete for {model_name}.\")\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    print(f\"{model_name} Model Performance for ml:\")\n",
    "    print(f'Train set: R2= {train_r2}, MAE = {train_mae}, RMSE = {train_rmse}')\n",
    "    print(f'Test set: R2 = {test_r2}, MAE = {test_mae}, RMSE = {test_rmse}')\n",
    "\n",
    "    #y_train = yref_train.values.ravel()\n",
    "    #y_test = yref_test.values.ravel()\n",
    "    #ysqm_train = ysqm_train.values.ravel()\n",
    "    #ysqm_test = ysqm_test.values.ravel()\n",
    "    #y_train_pred = ysqm_train + model.predict(X_train)\n",
    "    #y_test_pred = ysqm_test + model.predict(X_test)\n",
    "    #train_method_r2 = r2_score(y_train, ysqm_train)\n",
    "    #test_method_r2 = r2_score(y_test, ysqm_test)\n",
    "    #train_r2 = r2_score(y_train, y_train_pred)\n",
    "    #test_r2 = r2_score(y_test, y_test_pred)\n",
    "    #train_method_mae = mean_absolute_error(y_train, ysqm_train)\n",
    "    #test_method_mae = mean_absolute_error(y_test, ysqm_test)\n",
    "    #train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    #test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    #train_method_rmse = np.sqrt(mean_squared_error(y_train, ysqm_train))\n",
    "    #test_method_rmse = np.sqrt(mean_squared_error(y_test, ysqm_test))\n",
    "    #train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    #test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    #print(f\"Performance for PM7:\")\n",
    "    #print(f'Train set: R2= {train_method_r2}, MAE = {train_method_mae}, RMSE = {train_method_rmse}')\n",
    "    #print(f'Test set: R2 = {test_method_r2}, MAE = {test_method_mae}, RMSE = {test_method_rmse}')\n",
    "    #print(f\"{model_name} Model Performance for ysqm+ml:\")\n",
    "    #print(f'Train set: R2= {train_r2}, MAE = {train_mae}, RMSE = {train_rmse}')\n",
    "    #print(f'Test set: R2 = {test_r2}, MAE = {test_mae}, RMSE = {test_rmse}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_grid_search(model_name, model_class, X_train, X_test, ydiff_train, ydiff_test, yref_train, yref_test, ysqm_train, ysqm_test):\n",
    "    y_train = yref_train.values.ravel()\n",
    "    y_test = yref_test.values.ravel()\n",
    "    #y_train = ydiff_train.values.ravel()\n",
    "    #y_test = ydiff_test.values.ravel()\n",
    "    param_grid = regressor_configs[model_name]\n",
    "    print(f'Starting GridSearchCV for {model_name}...')\n",
    "    if model_name == 'CatBoostRegressor':\n",
    "        model = model_class(logging_level='silent')\n",
    "    else:\n",
    "        model = model_class()\n",
    "    grid_search = GridSearchCV(model_class(), param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "   \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    #joblib.dump(best_model, f'{model_name}_model.pkl')\n",
    "    return grid_search, best_model\n",
    "\n",
    "def print_results(model_name, best_model, X_train, X_test, ydiff_train, ydiff_test, yref_train, yref_test, ysqm_train, ysqm_test):\n",
    "    y_train = yref_train.values.ravel()\n",
    "    y_test = yref_test.values.ravel()\n",
    "    print(f\"GridSearchCV complete for {model_name}.\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score (MAE): {-grid_search.best_score_}\")\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    #y_train_pred = y_scaler.inverse_transform(y_train_pred.reshape(-1, 1))\n",
    "    #y_test_pred = y_scaler.inverse_transform(y_test_pred.reshape(-1, 1))\n",
    "    #y_train = y_scaler.inverse_transform(y_train)\n",
    "    #y_test = y_scaler.inverse_transform(y_test)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    print(f'{model_name} Model Performance:')\n",
    "    print(f'Train set: R2= {train_r2}, MAE = {train_mae}, RMSE = {train_rmse}')\n",
    "    print(f'Test set: R2 = {test_r2}, MAE = {test_mae}, RMSE = {test_rmse}')\n",
    "\n",
    "def display_results(y_train, y_train_pred):\n",
    "    sns.set_style(\"white\")\n",
    "    plt.figure(figsize= (5,5))\n",
    "    plt.xlabel(\"DFT Barrier Height $(kcal/mol)$\" , fontsize = 14)\n",
    "    plt.ylabel(\"ML Barrier Height $(kcal/mol)$\" , fontsize = 14)\n",
    "    plt.plot(y_train , y_train_pred , 'o' , color = \"red\" , markersize = 1 )\n",
    "    plt.plot(y_test , y_test_pred , 'o' , color = \"blue\" , markersize = 1 )\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name}_plot.pdf\",bbox_inches='tight')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "2e3a8d69-1891-4b20-8f07-28e7cb6b362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_configs = {\n",
    "    'Ridge': {\n",
    "        'alpha': [1e-1, 1, 10, 50],\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'alpha': [1e-1, 1, 10, 50],\n",
    "        'l1_ratio': [0, 0.25, 0.5, 0.75, 1],\n",
    "        'selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'alpha_1': [1e-6, 1e-5, 1e-4, 1e-3],\n",
    "        'alpha_2': [1e-6, 1e-5, 1e-4, 1e-3],\n",
    "        'lambda_1': [1e-6, 1e-5, 1e-4, 1e-3],\n",
    "        'lambda_2': [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "    },\n",
    "    'PassiveAggressiveRegressor': {\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10],\n",
    "        'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'epsilon': [0, 0.25, 0.5, 0.75, 1]\n",
    "    },\n",
    "    'HuberRegressor': {\n",
    "        'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10],\n",
    "        'epsilon': [1.35, 1.5, 1.75, 2]\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'n_neighbors': list(range(1, 31)),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        #'n_estimators': [10, 50, 100, 200, 500, 1000],\n",
    "        'max_depth': [1, 5, 10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        #'n_estimators': [10, 50, 100, 200, 500, 1000],\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 1],\n",
    "        'max_depth': [1, 3, 5, 7, 9],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'ExtraTreesRegressor': {\n",
    "        'n_estimators': [10, 50, 100, 200, 500, 1000],\n",
    "        'max_depth': [1, 5, 10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "       'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'n_estimators': [10, 50, 100, 200, 500, 1000],\n",
    "        'learning_rate': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'loss': ['linear', 'square', 'exponential']\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'max_depth': [1, 5, 10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': [1e-1, 1, 5, 10],\n",
    "        'epsilon': [0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'gamma': ['auto', 'scale'],\n",
    "        'degree': [2, 3, 4],\n",
    "        'coef0': [0, 0.5, 1]\n",
    "    },\n",
    "    'KernelRidge': {\n",
    "        'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf', 'laplacian', 'polynomial', 'sigmoid'],\n",
    "        #'gamma': ['auto', 'scale'],\n",
    "        #'degree': [2, 3, 4],\n",
    "        #'coef0': [0, 0.5, 1]\n",
    "    },\n",
    "    'GaussianProcessRegressor': {\n",
    "        'alpha': [1, 10, 20],\n",
    "        #'n_restarts_optimizer': [10, 20, 50],\n",
    "        'kernel': [\n",
    "            1.0 * RBF(length_scale=1.0),\n",
    "            1.0 * Matern(length_scale=1.0, nu=1.5),\n",
    "            1.0 * RationalQuadratic(length_scale=1.0, alpha=0.1),\n",
    "            1.0 * ExpSineSquared(length_scale=1.0, periodicity=3.0),\n",
    "            1.0 * DotProduct(sigma_0=1.0) + WhiteKernel(noise_level=1.0),\n",
    "            ConstantKernel(1.0) * RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0)            \n",
    "        ],\n",
    "    },\n",
    "    'XGBRegressor': {\n",
    "        #'n_estimators': [10, 50, 100, 200, 500, 1000],\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 1],\n",
    "        'max_depth': [1, 3, 5, 7, 9],\n",
    "        'subsample': [0.5, 0.75, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.75, 1.0],\n",
    "        'min_child_weight': [1, 5, 10]\n",
    "    },\n",
    "    'LGBMRegressor': {\n",
    "        'n_estimators': [10, 50, 100, 200, 500, 1000],\n",
    "        'learning_rate': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'max_depth': [1, 3, 5, 7, 9],\n",
    "        'num_leaves': [31, 63, 127],\n",
    "        'feature_fraction': [0.5, 0.75, 1.0],\n",
    "        'bagging_fraction': [0.5, 0.75, 1.0]\n",
    "    },\n",
    "    'CatBoostRegressor': {\n",
    "        #'iterations': [10, 50, 100, 200, 500, 1000],\n",
    "        'learning_rate': [1e-2, 1e-1],\n",
    "        #'depth': [1, 3, 5, 7, 9],\n",
    "        #'l2_leaf_reg': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10],\n",
    "        #'border_count': [32, 64, 128]\n",
    "    },\n",
    "    'NGBRegressor': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'minibatch_frac': [0.5, 0.75, 1.0],\n",
    "        'natural_gradient': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    #'Ridge': Ridge,\n",
    "    'ElasticNet': ElasticNet, #Select\n",
    "    #'BayesianRidge': BayesianRidge,\n",
    "    #'PassiveAggressiveRegressor': PassiveAggressiveRegressor,\n",
    "    #'HuberRegressor': HuberRegressor,\n",
    "    #'KNeighborsRegressor': KNeighborsRegressor,\n",
    "    'RandomForestRegressor': RandomForestRegressor, #Select\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor,#Select\n",
    "    #'ExtraTreesRegressor': ExtraTreesRegressor,\n",
    "    #'AdaBoostRegressor': AdaBoostRegressor,\n",
    "    #'DecisionTreeRegressor': DecisionTreeRegressor,\n",
    "    'SVR': SVR, #Select\n",
    "    #'KernelRidge': KernelRidge,\n",
    "    'GaussianProcessRegressor': GaussianProcessRegressor, #select\n",
    "    'XGBRegressor': XGBRegressor, #Select\n",
    "    #'LGBMRegressor': LGBMRegressor, \n",
    "    #'CatBoostRegressor': CatBoostRegressor,\n",
    "    #'NGBRegressor': NGBRegressor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "bcfd4973-42df-48f9-95a6-e31142afdac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV for ElasticNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.855e+03, tolerance: 2.731e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.561e+03, tolerance: 2.709e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.855e+03, tolerance: 2.731e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.225e+03, tolerance: 2.592e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.422e+03, tolerance: 2.764e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.918e+03, tolerance: 2.715e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.918e+03, tolerance: 2.715e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.422e+03, tolerance: 2.764e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.702e+03, tolerance: 2.731e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.766e+03, tolerance: 2.715e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+03, tolerance: 2.764e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.022e+03, tolerance: 2.592e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.561e+03, tolerance: 2.709e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.429e+03, tolerance: 2.709e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.702e+03, tolerance: 2.731e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.766e+03, tolerance: 2.715e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+03, tolerance: 2.764e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.429e+03, tolerance: 2.709e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.022e+03, tolerance: 2.592e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.225e+03, tolerance: 2.592e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.219e+04, tolerance: 2.731e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.225e+04, tolerance: 2.764e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.151e+04, tolerance: 2.592e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.220e+04, tolerance: 2.715e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.203e+04, tolerance: 2.709e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.220e+04, tolerance: 2.715e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.225e+04, tolerance: 2.764e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.203e+04, tolerance: 2.709e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.151e+04, tolerance: 2.592e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.219e+04, tolerance: 2.731e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.324e+04, tolerance: 2.715e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.329e+04, tolerance: 2.731e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.343e+04, tolerance: 2.764e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.329e+04, tolerance: 2.731e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.324e+04, tolerance: 2.715e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.317e+04, tolerance: 2.709e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.260e+04, tolerance: 2.592e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.343e+04, tolerance: 2.764e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.317e+04, tolerance: 2.709e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.260e+04, tolerance: 2.592e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV complete for ElasticNet.\n",
      "Best parameters: {'alpha': 0.1, 'l1_ratio': 1, 'selection': 'random'}\n",
      "Best cross-validation score (MAE): 3.679321670526787\n",
      "ElasticNet Model Performance:\n",
      "Train set: R2= 0.5310253430366259, MAE = 3.628589177502027, RMSE = 4.602666079051446\n",
      "Test set: R2 = 0.4630496812884044, MAE = 3.6004673977945205, RMSE = 4.582491557381443\n",
      "Starting GridSearchCV for RandomForestRegressor...\n",
      "GridSearchCV complete for RandomForestRegressor.\n",
      "Best parameters: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Best cross-validation score (MAE): 3.310588490143437\n",
      "RandomForestRegressor Model Performance:\n",
      "Train set: R2= 0.8670496862616999, MAE = 1.8836800121259587, RMSE = 2.450639697333343\n",
      "Test set: R2 = 0.5686721275029624, MAE = 3.105262314856985, RMSE = 4.107129370230521\n",
      "Starting GridSearchCV for GradientBoostingRegressor...\n",
      "GridSearchCV complete for GradientBoostingRegressor.\n",
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Best cross-validation score (MAE): 3.2749883655516165\n",
      "GradientBoostingRegressor Model Performance:\n",
      "Train set: R2= 0.8493239334789628, MAE = 2.0439497928580637, RMSE = 2.6088969082772038\n",
      "Test set: R2 = 0.5400107654741524, MAE = 3.176887638940107, RMSE = 4.241392412214504\n",
      "Starting GridSearchCV for SVR...\n",
      "GridSearchCV complete for SVR.\n",
      "Best parameters: {'C': 5, 'coef0': 0, 'degree': 2, 'epsilon': 0, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best cross-validation score (MAE): 3.339724674120943\n",
      "SVR Model Performance:\n",
      "Train set: R2= 0.712638750903307, MAE = 2.487497554963526, RMSE = 3.602872408718577\n",
      "Test set: R2 = 0.515582715881341, MAE = 3.1938659249056403, RMSE = 4.3525566918932945\n",
      "Starting GridSearchCV for GaussianProcessRegressor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:455: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:455: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:455: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:455: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:455: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:455: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k2__length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV complete for GaussianProcessRegressor.\n",
      "Best parameters: {'alpha': 10, 'kernel': 1**2 * RationalQuadratic(alpha=0.1, length_scale=1)}\n",
      "Best cross-validation score (MAE): 3.23150822731687\n",
      "GaussianProcessRegressor Model Performance:\n",
      "Train set: R2= 0.8555434104371311, MAE = 1.946103322311748, RMSE = 2.5544856053546683\n",
      "Test set: R2 = 0.5633253463466119, MAE = 3.101577305223809, RMSE = 4.132507148442215\n",
      "Starting GridSearchCV for XGBRegressor...\n",
      "GridSearchCV complete for XGBRegressor.\n",
      "Best parameters: {'colsample_bytree': 0.75, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.75}\n",
      "Best cross-validation score (MAE): 3.2259681155300783\n",
      "XGBRegressor Model Performance:\n",
      "Train set: R2= 0.8267223843691223, MAE = 2.191384040283009, RMSE = 2.7977313710542497\n",
      "Test set: R2 = 0.5718695781649816, MAE = 3.051545286077134, RMSE = 4.0918778960771185\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_class in models.items():\n",
    "    #best_model = fit_and_evaluate(model_name, model_class, X_train, X_test, ydiff_train, ydiff_test, yref_train, yref_test, ysqm_train, ysqm_test)\n",
    "    grid_search, best_model = run_grid_search(model_name, model_class, X_train, X_test, ydiff_train, ydiff_test, yref_train, yref_test, ysqm_train, ysqm_test)\n",
    "    print_results(model_name, best_model, X_train, X_test, ydiff_train, ydiff_test, yref_train, yref_test, ysqm_train, ysqm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb17e73-891f-4658-94c1-c7b86e559eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
